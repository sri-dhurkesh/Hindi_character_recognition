{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi_character_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCA7vEBulf5LQjaCQYGdL5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sri-dhurkesh/Hindi_character_recognition/blob/main/Hindi_character_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoCR48hwaPk_"
      },
      "source": [
        "### **Cloning the github repository**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG183mFHaCrq",
        "outputId": "2a750a07-29c5-423f-b57b-5d9c89ef13bb"
      },
      "source": [
        "!git clone https://github.com/sri-dhurkesh/Hindi_character_recognition.git"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Hindi_character_recognition'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 23 (delta 5), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (23/23), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Jwn0P8TGib"
      },
      "source": [
        "os.chdir('/content')"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVsPcv-1Sd2o",
        "outputId": "488a9c10-3d19-4106-f572-f5b4ef5e865d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "uname = \"darkshadow0\"\n",
        "!git config --global user.email 'svdhurkesh68@gmail.com'\n",
        "!git config --global user.name 'sri-dhurkesh'\n",
        "\n",
        "#Make a clone of github REPO\n",
        "!git clone https://sri-dhurkesh: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@github.com/sri-dhurkesh/Hindi_character_recognition.git"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Hindi_character_recognition'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 23 (delta 5), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (23/23), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya73ixDRTe_5",
        "outputId": "ef7f4478-fbeb-490d-ddca-68adef089717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9fGmDcxqg0y"
      },
      "source": [
        "### **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upQmK2CuqgQa"
      },
      "source": [
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "import sklearn.model_selection as sk\n",
        "import cv2\n",
        "import albumentations\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras import losses, optimizers, callbacks\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNXHA9HbYlDB",
        "outputId": "57b5e0d4-80d6-4089-8045-07a66b097786"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "ZipFile(\"/content/Hindi_character_recognition/data/training.zip\").extractall(\"/content/\")\n",
        "ZipFile(\"/content/Hindi_character_recognition/data/test.zip\").extractall(\"/content/\")\n",
        "print('Zip file is extracted')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file is extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbmwSkXTZefm",
        "outputId": "77c3b4d7-5367-486d-93ec-bfe2679c1565"
      },
      "source": [
        "train_data_0=os.listdir('/content/training/background/')\n",
        "train_data_1=os.listdir('/content/training/hi/')\n",
        "test_data=os.listdir('/content/test/')\n",
        "total=len(train_data_0)+len(train_data_1)\n",
        "print(f'bg: {len(train_data_0)}, {round((len(train_data_0)/total)*100)}% \\ntruth: {len(train_data_1)}, {round((len(train_data_1)/total)*100)}%\\n---------------------------\\nTest Data: {len(test_data)}')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bg: 4450, 76% \n",
            "truth: 1425, 24%\n",
            "---------------------------\n",
            "Test Data: 98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ1j6pjVte6i"
      },
      "source": [
        "# Read the path and target variable and store it in the list.\n",
        "x=[]\n",
        "y=[]\n",
        "test_x=[]\n",
        "for i in train_data_0:\n",
        "  x.append('/content/training/background/'+ i)\n",
        "  y.append(0)\n",
        "for j in train_data_1:\n",
        "  x.append('/content/training/hi/'+j)\n",
        "  y.append(1)\n",
        "for k in test_data:\n",
        "  test_x.append('/content/test/'+k)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S5aNDkayaLn"
      },
      "source": [
        "# shuffle the data.\n",
        "xn,yn=shuffle(np.array(x),np.array(y))\n",
        "xn=xn.tolist()\n",
        "yn=yn.tolist()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLW9-O_dyvV7",
        "outputId": "be534fcd-284d-4989-e040-11c8e6f8f055"
      },
      "source": [
        "# spilitting the data 70% training data and 30% testing data.\n",
        "train_x, val_x, train_y, val_y = sk.train_test_split(xn,yn,test_size=0.3, random_state=42)\n",
        "print('Training data:', len(train_x) )\n",
        "print('Training Label:', len(train_y))\n",
        "print('Validation Data:',len(val_x))\n",
        "print('Validation Label:', len(val_y))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: 4112\n",
            "Training Label: 4112\n",
            "Validation Data: 1763\n",
            "Validation Label: 1763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBZhYT7L4EX3"
      },
      "source": [
        "### **Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE4wnOg97eOr",
        "outputId": "b09fce51-0554-48ff-f7f3-9cfb9150360a"
      },
      "source": [
        "for i in train_x:\n",
        "  j=cv2.imread(train_x[0]).shape\n",
        "  if j[0]!=64 and j[1]!=64 and j[2]!=3:\n",
        "    print('True')\n",
        "print('All the images are of same size and dimension')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All the images are of same size and dimension\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5P1XC9x2py5"
      },
      "source": [
        "# Augmentation for training\n",
        "\n",
        "AUGMENTATIONS_TRAIN = Compose(\n",
        "    [\n",
        "        albumentations.ToFloat(max_value=255)      \n",
        "        \n",
        "    ]\n",
        ")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6XclHSq7YOt"
      },
      "source": [
        "#Augmentation for testing\n",
        "\n",
        "AUGMENTATIONS_TEST = Compose([\n",
        "    # CLAHE(p=1.0, clip_limit=2.0),\n",
        "    albumentations.ToFloat(max_value=255)\n",
        "])"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_g8PmCe8hrv"
      },
      "source": [
        "### **Data Generators using Sequence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5PS2SjJ8i9n"
      },
      "source": [
        "#from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q7EdXq28slY"
      },
      "source": [
        "class Hindi_Data_Sequence(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size, augmentations, shape):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augmentations\n",
        "        self.shape = shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return np.stack([\n",
        "            self.augment(image=cv2.resize(cv2.imread(x, 1), self.shape))[\"image\"] for x in batch_x\n",
        "        ], axis=0), np.array(batch_y)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q35MfSOE9y7"
      },
      "source": [
        "class Hindi_test_Data_Sequence(Sequence):\n",
        "    def __init__(self, x_set, batch_size, augmentations, shape):\n",
        "        self.x = x_set\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augmentations\n",
        "        self.shape = shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        #batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return np.stack([\n",
        "            self.augment(image=cv2.resize(cv2.imread(x, 1), self.shape))[\"image\"] for x in batch_x\n",
        "        ], axis=0)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okUYa2hq826I"
      },
      "source": [
        "train_gen=Hindi_Data_Sequence(train_x,train_y,16, augmentations=AUGMENTATIONS_TRAIN,shape=(64,64))\n",
        "val_gen=Hindi_Data_Sequence(val_x,val_y,16, augmentations=AUGMENTATIONS_TEST,shape=(64,64))\n",
        "test_gen=Hindi_test_Data_Sequence(test_x, 16, augmentations=AUGMENTATIONS_TEST,shape=(64,64))"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAyYz4Fg9LKQ",
        "outputId": "2e11bd5c-8f87-49e1-b6d3-b510a2a8c22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(64,64,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 64, 64, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 64, 64, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               1048704   \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,338,145\n",
            "Trainable params: 1,336,993\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ-bcqWH9dCX",
        "outputId": "6eff6795-1c4a-4660-820a-8708f42355dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.6, min_denta=0.00001)\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)\n",
        "file_path='/content/try1.h5'\n",
        "checkpoint= tf.keras.callbacks.ModelCheckpoint(file_path, monitor='val_loss',verbose=1,save_best_only=True)\n",
        "\n",
        "\n",
        "history = model.fit(train_gen, epochs=20, validation_data=val_gen,callbacks=[reduce_lr, es,checkpoint])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0271 - val_accuracy: 0.9926\n",
            "Epoch 2/20\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0349 - val_accuracy: 0.9898\n",
            "Epoch 3/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 0.0382 - val_accuracy: 0.9909\n",
            "Epoch 4/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0524 - val_accuracy: 0.9858\n",
            "Epoch 5/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.0305 - val_accuracy: 0.9909\n",
            "Epoch 6/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.0205 - val_accuracy: 0.9921\n",
            "Epoch 7/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9915\n",
            "Epoch 8/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0252 - val_accuracy: 0.9938\n",
            "Epoch 9/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0323 - val_accuracy: 0.9926\n",
            "Epoch 10/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0197 - val_accuracy: 0.9932\n",
            "Epoch 11/20\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9892\n",
            "Epoch 12/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0088 - accuracy: 0.9964 - val_loss: 0.0384 - val_accuracy: 0.9892\n",
            "Epoch 13/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.0301 - val_accuracy: 0.9926\n",
            "Epoch 14/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9915\n",
            "Epoch 15/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0363 - val_accuracy: 0.9904\n",
            "Epoch 16/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0365 - val_accuracy: 0.9904\n",
            "Epoch 17/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9915\n",
            "Epoch 18/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0366 - val_accuracy: 0.9898\n",
            "Epoch 19/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0364 - val_accuracy: 0.9898\n",
            "Epoch 20/20\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0428 - val_accuracy: 0.9887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_wGCgjnBzgJ"
      },
      "source": [
        "prediction = model.predict(test_gen)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC0sThWcGBci"
      },
      "source": [
        "prediction\n",
        "final=np.where(prediction>0.5,1,0)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IR-ifHUQVbb"
      },
      "source": [
        "### **Json Converter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg_hOy8EF3ii"
      },
      "source": [
        "import random\n",
        "import json\n",
        "# from utils.io import write_json\n",
        "\n",
        "\n",
        "def write_json(filename, result):\n",
        "    with open(filename, 'w') as outfile:\n",
        "        json.dump(result, outfile)\n",
        "\n",
        "def read_json(filename):\n",
        "    with open(filename, 'r') as outfile:\n",
        "        data =  json.load(outfile)\n",
        "    return data\n",
        "\n",
        "def generate_sample_file(filename,result,data):\n",
        "    res = {}\n",
        "    count=0\n",
        "    for i in data:\n",
        "        test_set = i\n",
        "        res[test_set] = int(result[count][0])\n",
        "        count+=1\n",
        "\n",
        "    write_json(filename, res)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    generate_sample_file('/content/Hindi_character_recognition/results.json',final,test_data)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE-yKcJgUJGc"
      },
      "source": [
        "### **Push the json file into directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6eY8VlGQai5"
      },
      "source": [
        "os.chdir('/content/Hindi_character_recognition')"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNigxQnsQhXK",
        "outputId": "6356e772-05ef-4adb-d93d-0ce6d20f857b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Hindi_character_recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "913RrcdKTuyH",
        "outputId": "ebe5b9d3-8d54-4e38-ad65-8dd4965ab97c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git status"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   results.json\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sky3IwJBQpZE"
      },
      "source": [
        "!git add ."
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au_li0NiQsCZ",
        "outputId": "d1fc8267-1322-4403-b969-f7cc3f499aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git commit -m \"json file is added\""
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 31a2641] json file is added\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
            " rewrite results.json (100%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy499b_SQ8hw",
        "outputId": "ed3c71aa-ac64-4bd7-a2c9-754245a4d8c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git push --all"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting objects: 3, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  33% (1/3)   \rCompressing objects:  66% (2/3)   \rCompressing objects: 100% (3/3)   \rCompressing objects: 100% (3/3), done.\n",
            "Writing objects:  33% (1/3)   \rWriting objects:  66% (2/3)   \rWriting objects: 100% (3/3)   \rWriting objects: 100% (3/3), 486 bytes | 486.00 KiB/s, done.\n",
            "Total 3 (delta 1), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/sri-dhurkesh/Hindi_character_recognition.git\n",
            "   82562dc..31a2641  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY6Xu0euScIJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}